{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjPGPXEp_7gf"
      },
      "source": [
        "# **Digital Image representation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaHb1sGnADTi"
      },
      "source": [
        "## **Grayscale image**\n",
        "\n",
        "A 2D grayscale image is an matrix $ I \\in \\mathbb{R}^{H \\times W}$. Where $H$ and $W$ denote the image height and width in pixels, respectively.\n",
        "\n",
        "Each entries $I_{ij}$ is a scalar represent the **intensity**(brightness) of the pixel located at the $i-th$ row and $j-th$ columns.\n",
        "\n",
        "$$\n",
        "I_{ij} \\in \\{0,1, \\dots, 255\\}\n",
        "$$\n",
        "\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we haven $3 \\times 3$ image with **9 pixels**\n",
        "\n",
        "$$\n",
        "I^{ 3 \\times 3}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0 & 127 & 255 \\\\\n",
        "50 & 180 & 220 \\\\\n",
        "10 & 60 & 90\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "OzO57EUY_SmU",
        "outputId": "40fb2104-6125-48b4-ea18-736344358db4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gray_img = np.array([\n",
        "    [0,   127, 255],\n",
        "    [50,  180, 220],\n",
        "    [10,   60,  90]\n",
        "], dtype=np.uint8)\n",
        "\n",
        "plt.imshow(gray_img, cmap='gray', vmin=0, vmax=255)\n",
        "plt.title(\"3×3 Grayscale Image\")\n",
        "plt.colorbar(label=\"Pixel Intensity\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76mV-M5lGI6e"
      },
      "source": [
        "## **Colored Image**\n",
        "\n",
        "A **color image** is a **3D matrix (tensor)** that stores three separate **grayscale layers**,  \n",
        "one for each color channel — **Red (R)**, **Green (G)**, and **Blue (B)**.\n",
        "\n",
        "Formally:\n",
        "\n",
        "$$\n",
        "I \\in \\mathbb{R}^{H \\times W \\times 3}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $H$: image height (rows)  \n",
        "* $W$: image width (columns)  \n",
        "* The 3rd dimension (size $=3$) corresponds to the three color channels.\n",
        "\n",
        "Where each entries (pixel) $\\mathbf{v}_{ij}$ is a **3D vector**  \n",
        "\n",
        "$$\n",
        "\\mathbf{v}_{ij} =\n",
        "\\begin{bmatrix}\n",
        "R_{ij}\\\\G_{ij}\\\\B_{ij}\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^3\n",
        "$$\n",
        "\n",
        "where each component gives that pixel’s red, green, and blue intensity:\n",
        "\n",
        "$$\n",
        "R_{ij},\\, G_{ij},\\, B_{ij} \\in \\{0,1,\\dots,255\\}.\n",
        "$$\n",
        "\n",
        "**Example** — $2\\times2$ RGB image:\n",
        "\n",
        "$$\n",
        "I^{2\\times2\\times3} =\n",
        "\\begin{bmatrix}\n",
        "[(255,0,0)] & [(0,255,0)] \\\\\n",
        "[(0,0,255)] & [(255,255,0)]\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Each entry like $[255, 0, 0]$ is the RGB vector for one pixel (pure red).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "bKsZYFCAV2Wc",
        "outputId": "5ccb8340-da2b-47fc-cb45-387cbd273cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (2, 2, 3)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rgb_img = np.array([\n",
        "    [[255,   0,   0], [0, 255,   0]],   # row 1: red, green\n",
        "    [[0,     0, 255], [255, 255, 0]]    # row 2: blue, yellow\n",
        "], dtype=np.uint8)\n",
        "\n",
        "print(\"Shape:\", rgb_img.shape)\n",
        "plt.imshow(rgb_img)\n",
        "plt.title(\"2×2 RGB Image\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_M8yid-1IjN"
      },
      "source": [
        "# Greyscale to RGB color image conversion\n",
        "\n",
        "\n",
        "We are trying to create a function $f$ that maps the 2D grayscale matrix to a 3D tensor.\n",
        "\n",
        "$$\n",
        "f(I_{\\text{gray}}) \\rightarrow I_{\\text{color}} \\quad\n",
        "f(\\mathbb{R}^{H \\times W}) \\rightarrow \\mathbb{R}^{H \\times W \\times 3}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtnQBrk9mPXh"
      },
      "source": [
        "### **Linear mapping from an RGB tensor to a grayscale image**\n",
        "\n",
        "Given a grayscale matrix $Y \\in  \\mathbb{R}^{H \\times W}$ with entries $y_{ij} \\in \\mathbb{R}$. We want to convert it to $ V \\in \\mathbb{R}^{H \\times W \\times 3}$ **3D** RGB tensor with entries $\\mathbf{v}_{ij} \\in \\mathbb{R}^3$.\n",
        "\n",
        "\n",
        "We can express each entries/pixels of $V$ as a **linear combination** of the corresponding entries 2D grayscal matrix $Y$\n",
        "\n",
        "$$\n",
        "y_{ij} \\; = \\; \\mathbf{w}^{\\top}\\mathbf{v}_{ij}\n",
        "\\; = \\;\n",
        "\\begin{bmatrix}\n",
        "w_R & w_G & w_B\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "R_{ij} \\\\ G_{ij} \\\\ B_{ij}\n",
        "\\end{bmatrix}\n",
        "= w_R R_{ij} + w_G G_{ij} + w_B B_{ij}\n",
        "$$\n",
        "\n",
        "This is the mapping **RGB Color -> Grayscale**. We want the reverse, that is we want to solve $\\mathbf{v}$ given $y = \\mathbf{w}^{\\top} \\mathbf{v}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "265e092f"
      },
      "source": [
        "## **Solving for $\\mathbf{v}$**\n",
        "\n",
        "While the mapping is helpful but we want to solve $\\mathbf{v}$, the expression $y = \\mathbf{w}^{\\top} \\mathbf{v}$ has infinite many solutions. And thus tell us nothing useful. So we need to find a more useful $\\mathbf{u}$ decomposition that gives more constraint:\n",
        "\n",
        "Any vector $\\mathbf{v}$ and a nonzero vector $\\mathbf{w}$ we can rewrite $\\mathbf{v}$ as\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{v}\n",
        "\\; &= \\;\n",
        "\\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) + \\left(\\mathbf{v} - \\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) \\right) \\\\[5pt]\n",
        "&= \\;\n",
        "\\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\mathbf{w}\\cdot\\mathbf{w}}\\,\\mathbf{w}\n",
        "+\n",
        "\\Big(\\mathbf{v} - \\frac{\\mathbf{v}\\cdot\\mathbf{w}}{\\mathbf{w}\\cdot\\mathbf{w}}\\,\\mathbf{w} \\Big) \\\\[5pt]\n",
        "&= \\;\n",
        "\\underbrace{\\frac{\\mathbf{w}^\\top \\mathbf{v}}{\\mathbf{w}^\\top \\mathbf{w}}\\,\\mathbf{w}}_{\\text{parallel to $\\mathbf{w}$}}\n",
        "+ \\underbrace{\\Big(\\mathbf{v} - \\frac{\\mathbf{w}^\\top \\mathbf{v}}{\\mathbf{w}^\\top \\mathbf{w}}\\,\\mathbf{w} \\Big)}_{\\text{orthogonal to $\\mathbf{w}$}}\n",
        "\\\\[5pt]\n",
        "&= \\;\n",
        "\\underbrace{\\frac{y}{\\mathbf{w}^\\top \\mathbf{w}}\\,\\mathbf{w}}_{\\text{brightness part of $\\mathbf{v}$}}\n",
        "+\n",
        "\\underbrace{\\Big(\\mathbf{v} - \\frac{y}{\\mathbf{w}^\\top \\mathbf{w}}\\,\\mathbf{w}\\Big)}_{\\text{the missing part of $\\mathbf{v}$}}\n",
        "&& \\text{ as y = $\\mathbf{w}^{\\top} \\mathbf{v}$}\\\\[5pt]\n",
        "&=\n",
        "\\underbrace{y}_{\\text{luminosity} \\\\ \\text{scalar} \\\\ y \\in \\{0,..,255\\} }\n",
        "\\underbrace{\\left({\\frac{1}{\\mathbf{w}^\\top \\mathbf{w}}}\\,\\mathbf{w} \\right)}_{\\text{3D vector}}\n",
        "+\n",
        "\\underbrace{\\left(\\mathbf{v} - y \\frac{1}{\\mathbf{w}^\\top \\mathbf{w}}\\,\\mathbf{w} \\right)}_{\\text{the missing part of $\\mathbf{v}$ } \\\\ \\text{without  luminosity (chroma)} } \\\\[5pt]\n",
        "&= \\mathbf{v}_{\\parallel} + \\mathbf{v}_{\\perp}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "This decomposition of show us that $\\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v})$ contains the brigthness or luminosity part of the RGB vector $\\mathbf{v}$ since $ y \\in Y$ is a scalar entries of matrix $Y$ representing brightness/luminosity.\n",
        "\n",
        "\n",
        "\n",
        "This also tell us another important constraint and confirm mathematically what we know $\\mathbf{w}$ is orthogonal to $\\mathbf{v}_{\\perp}$\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{\\top} \\left(\\mathbf{v} - \\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) \\right) = \\mathbf{w}^{\\top} \\mathbf{v}_{\\perp} = 0\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ING5VHxmHyMp"
      },
      "source": [
        "We can confirm that $\\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v})$ is the only responsible brightness component of $\\mathbf{v}$ by substutitng the above decomposition into $y = \\mathbf{w}^{\\top} \\mathbf{v}$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "y &= \\mathbf{w}^{\\top} \\mathbf{v} \\\\[5pt]\n",
        "&= \\mathbf{w}^{\\top} \\Big( \\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) + \\left(\\mathbf{v} - \\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) \\right) \\Big) \\\\[5pt]\n",
        "&= \\mathbf{w}^{\\top} \\left(\\mathbf{v}_{\\parallel} + \\mathbf{v}_{\\perp} \\right) \\\\[5pt]\n",
        "&= \\mathbf{w}^{\\top} \\mathbf{v}_{\\parallel} + \\mathbf{w}^{\\top} \\mathbf{v}_{\\perp} \\\\[5pt]\n",
        "&=  \\mathbf{w}^{\\top} \\mathbf{v}_{\\parallel}\n",
        " + 0  &&\\text{as $\\left(\\mathbf{v} - \\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) \\right) = \\mathbf{v}_{\\perp}$ is orhtogonal to $\\mathbf{w}$ }\\\\[5pt]\n",
        "&= \\mathbf{w}^{\\top} \\mathbf{v}_{\\parallel} && \\text{also show that $\\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) = \\mathbf{v}_{\\parallel}$  only contains luminosity part}\\\\[5pt]\n",
        "&= \\mathbf{w}^{\\top} \\frac{\\mathbf{w^{\\top} \\mathbf{v}}}{\\mathbf{w}^\\top \\mathbf{w}}\\,\\mathbf{w}\\\\[5pt]\n",
        "&= \\mathbf{w}^{\\top}\\mathbf{v} \\\\[5pt]\n",
        "&= y\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "The implication of this is that say we have some RGB vector $\\mathbf{v}$ with grayscale value $y = \\mathbf{w}^{\\top}\\mathbf{v}$.\n",
        "\n",
        "Now, suppose we add a vector $\\mathbf{c}$ (for \"chroma\" or color) that is perpendicular to $\\mathbf{w}$, meaning $\\mathbf{w}^{\\top}\\mathbf{c} = 0$.\n",
        "And we add $\\mathbf{c}$ to an arbitartry rgb vector and we define it as $\\mathbf{v}_{\\text{new}}$.\n",
        "\n",
        "$$\\mathbf{v}_{\\text{new}} = \\mathbf{v} + \\mathbf{c}$$\n",
        "\n",
        "\n",
        "\n",
        "notice that\n",
        "$$\n",
        "y_{\\text{new}} = \\mathbf{w}^{\\top}(\\mathbf{v} + \\mathbf{c}) = \\mathbf{w}^{\\top}\\mathbf{v} + \\mathbf{w}^{\\top}\\mathbf{c} = y + 0 = y\n",
        "$$\n",
        "\n",
        "so adding any vector perpendicular to $\\mathbf{w}$ doesn't change the grayscale value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f6b5a3"
      },
      "source": [
        "### **Understanding the Constraint Geometry**\n",
        "\n",
        "We have the constraint: $\\mathbf{w}^{\\top}\\mathbf{v}_{\\perp} = 0$, where\n",
        "$$\n",
        "\\mathbf{w} =\n",
        "\\begin{bmatrix}\n",
        "w_R \\\\ w_G \\\\ w_B\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf{v}_{\\perp} =\n",
        "\\begin{bmatrix}\n",
        "v_R \\\\ v_G \\\\ v_B\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Expanding the dot product gives:\n",
        "$$\n",
        "w_R v_R + w_G v_G + w_B v_B = 0.\n",
        "$$\n",
        "\n",
        "Solving for one variable (say $v_B$):\n",
        "$$\n",
        "v_B = -\\frac{w_R v_R + w_G v_G}{w_B}.\n",
        "$$\n",
        "\n",
        "Hence we can write:\n",
        "$$\n",
        "\\mathbf{v}_{\\perp} =\n",
        "\\begin{bmatrix}\n",
        "v_R \\\\\n",
        "v_G \\\\\n",
        "-\\dfrac{w_R v_R + w_G v_G}{w_B}\n",
        "\\end{bmatrix} =\n",
        "v_R\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "-\\dfrac{w_R}{w_B}\n",
        "\\end{bmatrix}\n",
        "+\n",
        "v_G\n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "-\\dfrac{w_G}{w_B}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "Here, $v_R$ and $v_G$ are **free parameters** that can take any real values, while $v_B$ is determined by the orthogonality constraint.\n",
        "Thus, $\\mathbf{v}_{\\perp}$ lies in a **2-dimensional subspace** of $\\mathbb{R}^3$, and can be expressed as a **linear combination of two independent basis vectors**.\n",
        "\n",
        "Choose any two independent vectors\n",
        "\n",
        "$$\n",
        "\\mathbf{u}_1 =\n",
        "\\begin{bmatrix}\n",
        "u_{1R} \\\\ u_{1G} \\\\ u_{1B}\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf{u}_2 =\n",
        "\\begin{bmatrix}\n",
        "u_{2R} \\\\ u_{2G} \\\\ u_{2B}\n",
        "\\end{bmatrix} \\in \\mathbb{R}^3\n",
        "\\quad \\text{such that} \\quad\n",
        "\\mathbf{w}^\\top \\mathbf{u}_1 = \\mathbf{w}^\\top \\mathbf{u}_2 = 0,\n",
        "$$\n",
        "\n",
        "and represent every possible chroma vector $\\mathbf{v}_{\\perp}$ as\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_{\\perp} = \\alpha\\,\\mathbf{u}_1 + \\beta\\,\\mathbf{u}_2, \\qquad \\alpha, \\beta \\in \\mathbb{R}.\n",
        "$$\n",
        "\n",
        "The set of all such vectors orthogonal to $\\mathbf{w}$ (for one pixel or $\\mathbf{v}_{ij}$ )is\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^\\perp = \\{ \\mathbf{v}_{\\perp} \\in \\mathbb{R}^3 : \\mathbf{w}^\\top \\mathbf{v}_{\\perp} = 0 \\}\n",
        "$$\n",
        "\n",
        "which forms a **plane through the origin  a 2-dimensional subspace** of $\\mathbb{R}^3$.\n",
        "\n",
        "$$\n",
        "\\dim(\\mathbf{w}^\\perp) = 2\n",
        "$$\n",
        "\n",
        "Then for the entire image\n",
        "$$\n",
        "\\dim((\\mathbf{w}^\\perp)^{H\\times W}) = 2 \\times (H \\times W).\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEstbe01oI5Y"
      },
      "source": [
        "Therefore we can represent RGB vector $\\mathbf{v}$ as\n",
        "\n",
        "$$\n",
        "\\mathbf{v} = \\frac{y_{ij}}{\\mathbf{w}^{\\top}\\mathbf{w}}\\mathbf{w} + \\underbrace{\\alpha\\,\\mathbf{u}_1 + \\beta\\,\\mathbf{u}_2}_{\\mathbf{v}_{\\perp}}\n",
        "$$\n",
        "\n",
        "where $\\alpha, \\beta \\in \\mathbb{R}$ are free parameters. Notice it can take any real values, which mean $\\mathbf{v}$ can go outside of the RGB range\n",
        "$0 \\le R, G, B \\le 255$. Thus we will use lab space\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XTGXYDwYbvZ"
      },
      "source": [
        "# **Using Supervised Learning Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8jgwCLPwMsn"
      },
      "source": [
        "## **Training phasef**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwzUcmvGwLkM"
      },
      "source": [
        "Given a RGB colored image 3D tensor $\\mathbf V \\in \\mathbb{R}^{H\\times W\\times 3},$\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathbf V =\n",
        "\\begin{bmatrix}\n",
        "\\mathbf v_{11} & \\mathbf v_{12} & \\dots & \\mathbf v_{1W}\\\\[3pt]\n",
        "\\mathbf v_{21} & \\mathbf v_{22} & \\dots & \\mathbf v_{2W}\\\\[3pt]\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\[3pt]\n",
        "\\mathbf v_{H1} & \\mathbf v_{H2} & \\dots & \\mathbf v_{HW}\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf v_{ij} =\n",
        "\\begin{bmatrix}\n",
        "R_{ij}\\\\[3pt]G_{ij}\\\\[3pt]B_{ij}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "We extract the whole grayscale $Y \\in \\mathbb{R}^{H\\times W}$ matrix at once\n",
        "\n",
        "$$\n",
        "Y =\n",
        "\\begin{bmatrix}\n",
        "y_{11} & y_{12} & \\dots & y_{1W}\\\\\n",
        "y_{21} & y_{22} & \\dots & y_{2W}\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "y_{H1} & y_{H2} & \\dots & y_{HW}\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^{H\\times W}.\n",
        "$$\n",
        "\n",
        "using definiton\n",
        "\n",
        "$$\n",
        "Y=[y_{ij}]=[\\mathbf w^\\top\\mathbf v_{ij}],\n",
        "$$\n",
        "\n",
        "Then we select $m$ filters (or kernels) of our choice,\n",
        "$$\n",
        "K^{(1)}, K^{(2)}, \\dots, K^{(m)},\n",
        "$$\n",
        "\n",
        "where each $K^{(k)}$ is a matrix of size $p \\times q$. And $K^{(k)}_{pq}$ is the  **entry** of $K^{(k)}$\n",
        "\n",
        "$$\n",
        "K^{(k)} =\n",
        "\\begin{bmatrix}\n",
        "K^{(k)}_{11} & K^{(k)}_{12} & \\dots & K^{(k)}_{1q} \\\\\n",
        " K^{(k)}_{21} & K^{(k)}_{22} & \\dots & K^{(k)}_{2q} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " K^{(k)}_{p1} & K^{(k)}_{p2} & \\dots & K^{(k)}_{pq}\n",
        "\\end{bmatrix}  \n",
        "\\qquad\n",
        "\\color{grey}\n",
        "{\n",
        "\\text{ex: }\n",
        "K^{(k)} =\n",
        "\\begin{bmatrix}\n",
        "0 & -1 & 0 \\\\[3pt]\n",
        "-1 & 4 & -1 \\\\[3pt]\n",
        "0 & -1 & 0\n",
        "\\end{bmatrix}.\n",
        "}\n",
        "$$\n",
        "\n",
        "For every local submatrix $Y_{ij}$ of the luminance matrix $Y$\n",
        "\n",
        "we compute its **feature vector** $\\mathbf z_{ij}\\in\\mathbb{R}^m$ by taking the inner product of each filter with that patch:\n",
        "\n",
        "$$\n",
        "\\mathbf z_{ij} =\n",
        "\\begin{bmatrix}\n",
        "z_{1} =\\langle K^{(1)}, Y_{ij} \\rangle_F \\\\[4pt]\n",
        "z_{2} = \\langle K^{(2)}, Y_{ij} \\rangle_F \\\\[4pt]\n",
        "\\vdots \\\\[4pt]\n",
        "z_{m} = \\langle K^{(m)}, Y_{ij} \\rangle_F\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "> where\n",
        "\n",
        ">$$\n",
        "\\langle K^{(k)}, Y_{ij}\\rangle_F\n",
        "\\;=\\;\n",
        "\\sum_{p,q} K^{(k)}_{pq}\\,Y_{pq}\n",
        "\\; = \\;\n",
        "{\\scriptsize\n",
        "\\sum_{\\text{all entries}} \\left(\n",
        "\\begin{bmatrix}\n",
        "K^{(k)}_{11}Y_{11}  & \\cdots & K^{(k)}_{1q}Y_{1q}\\\\\n",
        "K^{(k)}_{21}Y_{21}  & \\cdots & K^{(k)}_{2q}Y_{2q}\\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "K^{(k)}_{p1}Y_{p1}  & \\cdots & K^{(k)}_{pq}Y_{pq}\n",
        "\\end{bmatrix}\n",
        "\\right)\n",
        "}\n",
        "\\; = \\;\n",
        "K^{(k)}_{11}Y_{i1} + K^{(k)}_{12}Y_{i2} + \\dots + K^{(k)}_{pq}Y_{iq}\n",
        "$$\n",
        "\n",
        "Each component $z_k(i,j)=\\langle K^{(k)},Y_{ij}\\rangle$ tell us  **how strongly** the local patch $Y_{ij}$ matches the luminance pattern encoded by the $k$-th filter $K^{(k)}$.\n",
        "\n",
        "Then for every submatrices $\\mathbf{v}_{ij}$ we computed the predicted RGB vector\n",
        "\n",
        "$$\n",
        "\\widehat{\\mathbf v}_{ij}\n",
        "\\; = \\;\n",
        "\\underbrace{\\frac{y_{ij}}{\\mathbf w^\\top\\mathbf w}\\mathbf w}_{\\mathbf{v}_{\\parallel}}\n",
        "\\; + \\;\n",
        "\\underbrace{\\left[ (\\mathbf a^\\top \\mathbf z_{ij})\\,\\mathbf u_1\n",
        "+ (\\mathbf b^\\top \\mathbf z_{ij})\\,\\mathbf u_2 \\right]}_{\\mathbf{v}_{\\perp}}\n",
        "$$\n",
        "\n",
        ">> with unknown parameter vector $\\mathbf{a}, \\; \\mathbf{b} \\in \\mathbb{R}^{m}$ that we want to solve for that give us $\\widehat{\\mathbf v}_{ij} \\approx \\mathbf{v}_{ij}$. And  $\\mathbf{u}_1, \\; \\mathbf{u}_2$ satisfy the constraint  $\\mathbf{w}^{\\top} \\mathbf{u}_1 =  0$ and $\\mathbf{w}^{\\top} \\mathbf{u}_2 = 0$. (orthonormal basis for the chroma plane orthogonal to $\\mathbf{w}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM97jpTOMTil"
      },
      "source": [
        "We can define the predicted chroma scalars as:\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} =\\mathbf a^\\top\\mathbf z_{ij},\\qquad\n",
        "\\beta_{ij} =\\mathbf b^\\top\\mathbf z_{ij}. \\\\\n",
        "\\mathbf v_\\perp\\; = \\; \\alpha \\,\\mathbf u_1 + \\beta \\,\\mathbf u_2  \n",
        "$$\n",
        "\n",
        "Then $\\alpha^{\\text{true}}, \\beta^{\\text{true}}$  are\n",
        "\n",
        "$$\n",
        "\\alpha^{\\text{true}}_{ij}\n",
        "= \\mathbf u_1^\\top \\mathbf v_{ij}\n",
        "=\\mathbf u_1^\\top\\Big(\\mathbf v_{ij}-\\tfrac{y_{ij}}{\\mathbf w^\\top\\mathbf w}\\mathbf w\\Big) \\\\\n",
        "\\beta^{\\text{true}}_{ij}\n",
        "=  \\mathbf u_2^\\top \\mathbf v_{ij}\n",
        "= \\mathbf u_2^\\top\\Big(\\mathbf v_{ij}-\\tfrac{y_{ij}}{\\mathbf w^\\top\\mathbf w}\\mathbf w\\Big).\n",
        "$$\n",
        "\n",
        "<details>\n",
        "  <summary><b>derivation</b></summary>\n",
        "\n",
        "The **true** chroma coefficients, $\\alpha^{\\text{true}}, \\beta^{\\text{true}}$ can be find by solving for $\\alpha$ and $\\beta$.\n",
        "Say to solve for $\\alpha$  we need to take the dot product of $\\mathbf u_1$ of the whole expression\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf v_\\perp &=  \\alpha \\,\\mathbf u_1 + \\beta \\,\\mathbf u_2  \\\\[4pt]\n",
        "(\\mathbf u_1^\\top)  \\mathbf v_\\perp &= \\alpha (\\mathbf u_1^\\top \\mathbf u_1) + \\beta (\\mathbf u_1^\\top \\mathbf u_2) \\\\[4pt]\n",
        "\\mathbf u_1^\\top  \\mathbf v_\\perp  &= \\alpha (\\mathbf u_1^\\top \\mathbf u_1) + \\beta \\cdot 0 && \\text{ $\\mathbf{u_1}, \\mathbf{u_2}$ are orthogonal} \\\\[4pt]\n",
        "\\alpha &= \\frac{\\mathbf u_1^\\top  \\mathbf v_\\perp}{(\\mathbf u_1^\\top \\mathbf u_1) }\n",
        "&& \\text{$(\\mathbf u_1^\\top \\mathbf u_1)$, $\\mathbf{u_1^\\top} > 0$ are scalar} \\\\[4pt]\n",
        "\\alpha &= \\frac{\\mathbf u_1^\\top}{\\|\\mathbf u_1\\|^2 }  \\mathbf{v}_\\perp\n",
        "&& (\\mathbf{u}_1^{\\top} \\mathbf{u}_1 = \\|\\mathbf u_1\\|^2 )\n",
        " \\\\[4pt]\n",
        "\\alpha &= \\mathbf{u}_1^{\\top} \\mathbf{v}_\\perp\n",
        "&& \\text{if $\\mathbf{u}_2$ is normalized $\\implies  \\|\\mathbf u_1\\|^2 = 1$ } \\\\[4pt]\n",
        "\\implies \\beta  &= \\mathbf{u}_2^{\\top} \\mathbf{v}_\\perp\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Since\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_{\\perp}\n",
        "\\; = \\;\n",
        "\\left(\\mathbf{v} - \\operatorname{proj}_{\\mathbf{w}}(\\mathbf{v}) \\right)\n",
        "\\; = \\;\n",
        "\\Big(\\mathbf{v} - \\frac{y}{\\mathbf{w}^\\top \\mathbf{w}} \\mathbf{w} \\Big)\n",
        "$$\n",
        "\n",
        "Thus  $\\alpha^{\\text{true}}, \\beta^{\\text{true}}$  are\n",
        "\n",
        "$$\n",
        "\\alpha^{\\text{true}}_{ij}\n",
        "= \\mathbf u_1^\\top \\mathbf v_{ij}\n",
        "=\\mathbf u_1^\\top\\Big(\\mathbf v_{ij}-\\tfrac{y_{ij}}{\\mathbf w^\\top\\mathbf w}\\mathbf w\\Big) \\\\\n",
        "\\beta^{\\text{true}}_{ij}\n",
        "=  \\mathbf u_2^\\top \\mathbf v_{ij}\n",
        "= \\mathbf u_2^\\top\\Big(\\mathbf v_{ij}-\\tfrac{y_{ij}}{\\mathbf w^\\top\\mathbf w}\\mathbf w\\Big).\n",
        "$$\n",
        "</details>\n",
        "\n",
        "\n",
        "Then the for a single reconstruction $\\mathbf{v}_{ij}$ error\n",
        "\n",
        "$$\n",
        "e_{ij} = \\big\\| \\mathbf v_{ij} - \\widehat{\\mathbf v}_{ij} \\big\\|^2.\n",
        "$$\n",
        "\n",
        "Let the total number of valid submatrices be $N=(H-p+1)(W-q+1)$\n",
        "\n",
        "$$\n",
        "Z=\\begin{bmatrix}\\mathbf z_1^\\top\\\\ \\vdots\\\\ \\mathbf z_N^\\top\\end{bmatrix}\\in\\mathbb R^{N\\times m},\\quad\n",
        "\\boldsymbol\\alpha^{\\text{true}}=\\begin{bmatrix}\\alpha^{\\text{true}}_1\\\\ \\vdots\\\\ \\alpha^{\\text{true}}_N\\end{bmatrix},\\quad\n",
        "\\boldsymbol\\beta^{\\text{true}}=\\begin{bmatrix}\\beta^{\\text{true}}_1\\\\ \\vdots\\\\ \\beta^{\\text{true}}_N\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Then the total reconstruction error is:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E(\\mathbf a,\\mathbf b)\n",
        "&=\n",
        "\\sum_{i,j}\n",
        "\\|\\mathbf v_{ij}-\\widehat{\\mathbf v}_{ij}\\|^2 \\\\[4pt]\n",
        "&=\n",
        "\\|\\alpha^{\\text{true}} - Z\\mathbf a\\|^2\n",
        "+\\|\\beta^{\\text{true}} - Z\\mathbf b\\|^2.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus we want to find the vector $\\mathbf{a}, \\mathbf{b}$ that minimize the total error $E(\\mathbf a,\\mathbf b)$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3e95ccd"
      },
      "source": [
        "## **In Lab-space**\n",
        "\n",
        "\n",
        "In Lab space, we define\n",
        "$$\n",
        "\\mathbf v_{ij}^{(Lab)} =\n",
        "f_{\\text{RGB}\\to\\text{Lab}}(\\mathbf v_{ij})\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "L_{ij}\\\\[3pt]\n",
        "a_{ij}\\\\[3pt]\n",
        "b_{ij}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Then\n",
        "$$\n",
        "\\mathbf w =\n",
        "\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}\n",
        "\\quad (\\text{for } L),\n",
        "\\qquad\n",
        "\\mathbf u_1 =\n",
        "\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix},\n",
        "\\quad\n",
        "\\mathbf u_2 =\n",
        "\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "That gives\n",
        "$$\n",
        "y_{ij} = \\mathbf w^\\top \\mathbf v_{ij}^{(Lab)} = L_{ij},\n",
        "\\qquad\n",
        "\\frac{y_{ij}}{\\mathbf w^\\top \\mathbf w}\\mathbf w\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "L_{ij}\\\\0\\\\0\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Thus the predictor becomes\n",
        "$$\n",
        "\\widehat{\\mathbf v}_{ij}^{(Lab)} =\n",
        "\\begin{bmatrix}\n",
        "L_{ij}\\\\\n",
        "\\mathbf a^\\top \\mathbf z_{ij}\\\\\n",
        "\\mathbf b^\\top \\mathbf z_{ij}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Subtracting,\n",
        "$$\n",
        "\\mathbf v_{ij}^{(Lab)} - \\widehat{\\mathbf v}_{ij}^{(Lab)}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0\\\\\n",
        "a_{ij} - \\mathbf a^\\top \\mathbf z_{ij}\\\\\n",
        "b_{ij} - \\mathbf b^\\top \\mathbf z_{ij}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "(a_{ij}-\\mathbf a^\\top\\mathbf z_{ij})\\,\\mathbf u_1\n",
        "+\n",
        "(b_{ij}-\\mathbf b^\\top\\mathbf z_{ij})\\,\\mathbf u_2.\n",
        "$$\n",
        "\n",
        "Then\n",
        "$$\n",
        "\\big\\|\n",
        "\\mathbf v_{ij}^{(Lab)} - \\widehat{\\mathbf v}_{ij}^{(Lab)}\n",
        "\\big\\|^2\n",
        "=\n",
        "(a_{ij}-\\mathbf a^\\top\\mathbf z_{ij})^2\n",
        "+\n",
        "(b_{ij}-\\mathbf b^\\top\\mathbf z_{ij})^2.\n",
        "$$\n",
        "\n",
        "Hence the goal is\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf a,\\mathbf b}\\ E(\\mathbf a,\\mathbf b)\n",
        "=\\min_{\\mathbf a,\\mathbf b}\\big(\\|a^{true}-Z\\mathbf a\\|^2+\\|b^{true}-Z\\mathbf b\\|^2\\big).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH2KWV5yHs0g"
      },
      "source": [
        "# **Converting to CNN**\n",
        "Our goal is to make our prediction ($\\widehat{\\mathbf{v}}_{ij}$) match the truth ($\\mathbf{v}_{ij}$).\n",
        "\n",
        "Let's look at the problem with SLR, we measure the error using **MSE**\n",
        "\n",
        "\n",
        "$$\n",
        "e_{ij} = e_{\\alpha} + e_{\\beta} = (\\alpha^{\\text{true}}_{ij} - \\alpha_{ij})^2 + (\\beta^{\\text{true}}_{ij} - \\beta_{ij})^2\n",
        "$$\n",
        "\n",
        "We obviously want to make $e_{ij}$  $\\approx 0$ as much as possible, but we can't solve this directly. But we can make changes to $\\mathbf{a,b}$ and $K^{(k)}$ to make $e_{ij}$ smaller.\n",
        "\n",
        "We want to see how changes to parameters affect the changes of $e_{ij}$, this is where **derivative**, in this case **(partial derivative)** comes in.\n",
        "\n",
        "Since  \n",
        "$$\n",
        "\\frac{\\partial}{\\partial e_{ij}} = \\frac{\\partial}{\\partial e_{\\alpha}} + \\frac{\\partial}{\\partial e_{\\beta}}\n",
        "$$\n",
        "\n",
        "We can start looking at $\\frac{\\partial}{\\partial e_{\\alpha}}$ first.\n",
        "\n",
        "\n",
        "Our prediction parameter $\\alpha_{ij}$ is defined as\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} = a_1 z_1 + a_2 z_2 + \\dots + \\color{grey}{a_k z_k} + \\dots + a_m z_m\n",
        "$$\n",
        "\n",
        "While the actual\n",
        "\n",
        "$$\n",
        "\\alpha^{\\text{true}}_{ij}\n",
        "= \\mathbf u_1^\\top \\mathbf v_{ij}\n",
        "=\\mathbf u_1^\\top\\Big(\\mathbf v_{ij}-\\tfrac{y_{ij}}{\\mathbf w^\\top\\mathbf w}\\mathbf w\\Big)\n",
        "$$\n",
        "\n",
        "If we want to see how changes to $\\color{grey}{a_k}$ affect $e_{ij}$ we can use the chain rule to find $\\frac{\\partial e_{\\alpha}}{\\partial a_k}$\n",
        "\n",
        ">> **Chain rule**:\n",
        ">> $$\n",
        "\\frac{d}{dx}\\!\\left[ {\\color{blue}{f}} \\color{blue}{ ( } \\color{red}{g(x)} \\color{blue}{ )}   \\right]\n",
        "= {\\color{blue}{f'}}{\\color{blue}{(}}\\color{red}{g(x)}{\\color{blue}{)}}\n",
        "\\, \\cdot \\color{red}{g'(x)}\n",
        "\\color{grey} {\\; = \\; \\frac{df}{dg} \\cdot \\frac{dg}{dx}}\n",
        "$$\n",
        "\n",
        "Then\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e_{\\alpha}}{\\partial a_k}\n",
        "\\; = \\;\n",
        "\\underbrace{\n",
        "{\\color{blue}{2}}\\,\n",
        "{\\color{blue}{(}}\\color{red}{\\alpha^{\\text{true}}_{ij} - \\alpha_{ij}}{\\color{blue}{)}}^{\\color{blue}{\\,2-1}}\n",
        "}_{\\text{First part}}\n",
        "\\cdot\n",
        "\\underbrace{\n",
        "\\color{red}{\\frac{\\partial}{\\partial a_k}\\left[ \\alpha^{\\text{true}}_{ij} - \\alpha_{ij} \\right]}\n",
        "}_{\\text{Second part}}\n",
        "\\; = \\;\n",
        "{\\color{blue}{2}}\\,\n",
        "{\\color{blue}{(}}\\color{red}{\\alpha^{\\text{true}}_{ij} - \\alpha_{ij}}{\\color{blue}{)}}\n",
        "\\cdot\n",
        "\\color{red}{(-z_k)}\n",
        "$$\n",
        "\n",
        "<details>\n",
        "  <summary><b>Expanding $\\text{Second part}$</b></summary>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\color{red}{\\frac{\\partial}{\\partial a_k} \\left[ \\alpha^{\\text{true}}_{ij} - \\alpha_{ij} \\right]}\n",
        "\\; &= \\;\n",
        "\\frac{\\partial}{\\partial a_k} \\alpha^{\\text{true}}_{ij}  -  \n",
        "\\frac{\\partial}{\\partial a_k} \\alpha_{ij}\n",
        "\\\\[4pt]\n",
        "\\; &= \\;\n",
        "\\frac{\\partial}{\\partial a_k} u_1^\\top \\mathbf v_{ij}\n",
        "-\n",
        "\\frac{\\partial}{\\partial a_k} \\left[  a_1 z_1 + a_2 z_2 + \\dots + a_k z_k + \\dots + a_m z_m \\right]\n",
        "\\\\[4pt]\n",
        "\\; &= \\;\n",
        "0\n",
        "-\n",
        "\\frac{\\partial}{\\partial a_k} \\left[  a_1 z_1 + a_2 z_2 + \\dots + a_k z_k + \\dots + a_m z_m \\right]\n",
        " \\\\[5pt]\n",
        "\\; &= \\;\n",
        "0\n",
        "-\n",
        "\\frac{\\partial}{\\partial a_k} \\left[ a_k z_k \\right]\n",
        "\\\\[4pt]\n",
        "&= \\color{red}{-z_k}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRXm9cCveTwa"
      },
      "source": [
        "$e_{\\alpha}$ depends on $\\alpha_{ij}$.\n",
        "And our  prediction $\\alpha_{ij}$ depends on $\\mathbf{z}_{ij} $ (say specially $z_k$).\n",
        "\n",
        "\n",
        "$$\n",
        "{\\small\n",
        "\\mathbf z_{ij} =\n",
        "\\begin{bmatrix}\n",
        "z_{1} =\\langle K^{(1)}, Y_{ij} \\rangle_F \\\\[4pt]\n",
        "\\vdots \\\\[4pt]\n",
        "z_{k} = \\langle K^{(k)}, Y_{ij} \\rangle_F \\\\[4pt]\n",
        "\\vdots \\\\[4pt]\n",
        "z_{m} = \\langle K^{(m)}, Y_{ij} \\rangle_F\n",
        "\\end{bmatrix}\n",
        "\\qquad\n",
        "z_k = \\langle K^{(k)}, Y_{ij} \\rangle_F =  \\sum K^{(k)}_{pq} \\cdot Y_{ij}(p,q)\n",
        "},\n",
        "$$\n",
        "\n",
        "\n",
        "And our $z_k$ depends on the kernal entry $K^{(k)}_{pq}$ (since $Y_{ij}$ is given).\n",
        "\n",
        "Thus then we want to find $\\frac{\\partial e_{\\alpha}}{\\partial K^{(k)}_{pq}}$.And we know the link is $K^{(k)}_{pq} \\rightarrow z_k \\rightarrow \\alpha_{ij} \\rightarrow e_{\\alpha}$\n",
        "\n",
        "So\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial e_{\\alpha}}{\\partial K^{(k)}_{pq}}\n",
        "&=\n",
        "\\frac{\\partial e_{\\alpha}}{\\partial \\alpha_{ij}}\n",
        "\\cdot\n",
        "\\frac{\\partial \\alpha_{ij}}{\\partial z_k}\n",
        "\\cdot\n",
        "\\frac{\\partial z_k}{\\partial K^{(k)}_{pq}} \\\\[4pt]\n",
        "&=\n",
        "\\left( -2(\\alpha^{\\text{true}}_{ij} - \\alpha_{ij}) \\right)\n",
        "\\cdot\n",
        "\\left( a_k \\right)\n",
        "\\cdot\n",
        "\\frac{\\partial z_k}{\\partial K^{(k)}_{pq}} \\\\[4pt]\n",
        "&=\n",
        "\\left( -2(\\alpha^{\\text{true}}_{ij} - \\alpha_{ij}) \\right)\\left( a_k \\right)\n",
        "\\cdot\n",
        "\\frac{\\partial }{\\partial K^{(k)}_{pq}}\n",
        "\\big[K^{(k)}_{11} \\cdot Y_{ij}(1,1)) + \\dots +  \\underbrace{(K^{(k)}_{pq} \\cdot Y_{ij}(p,q))}_{\\text{our term}} + \\dots\n",
        "\\big] \\\\[4pt]\n",
        "&=\n",
        "\\left( -2(\\alpha^{\\text{true}}_{ij} - \\alpha_{ij}) \\right)\\left( a_k \\right)\n",
        "\\cdot\n",
        "\\frac{\\partial}{\\partial K^{(k)}_{pq}} \\left[ K^{(k)}_{pq} \\cdot Y_{ij}(p,q) \\right]\\\\[4pt]\n",
        "&=\n",
        "\\left( -2(\\alpha^{\\text{true}}_{ij} - \\alpha_{ij}) \\right)\\left( a_k \\right)\n",
        "\\cdot\n",
        "\\left( Y_{ij}(p,q) \\right)\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEEJ1tYVyrjr"
      },
      "source": [
        "Thus\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e_{\\alpha}}{\\partial K^{(k)}_{pq}}\n",
        "=\n",
        "\\left( -2(\\alpha^{\\text{true}}_{ij} - \\alpha_{ij}) \\right)\n",
        "a_k \\;\n",
        "Y_{ij}(p,q)\n",
        "$$\n",
        "\n",
        "This mean for $\\frac{\\partial e_{\\alpha}}{\\partial K^{(k)}_{pq}}$ The link is similar $K^{(k)}_{pq} \\rightarrow z_k \\rightarrow \\beta_{ij} \\rightarrow e_{\\beta}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e_{\\beta}}{\\partial K^{(k)}_{pq}}\n",
        "=\n",
        "\\left( -2(\\beta^{\\text{true}}_{ij} - \\beta_{ij}) \\right)\n",
        "b_k \\;\n",
        "Y_{ij}(p,q)\n",
        "$$\n",
        "\n",
        "THen for the total error $e_{ij}$ (**Entry wise form: (Scalar)**)\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial e_{ij}}{\\partial K^{(k)}_{pq}}\n",
        "&= \\frac{\\partial e_{\\alpha}}{\\partial K^{(k)}_{pq}} + \\frac{\\partial e_{\\beta}}{\\partial K^{(k)}_{pq}}\n",
        "\\frac{\\partial e_{ij}}{\\partial K^{(k)}_{pq}} \\\\[4pt]\n",
        "&= \\;\n",
        "-2\\Big[\n",
        "(\\alpha^{\\text{true}}_{ij} - \\alpha_{ij})\\; a_k\n",
        "+\n",
        "(\\beta^{\\text{true}}_{ij} - \\beta_{ij})\\; b_k\n",
        "\\Big]\n",
        "Y_{ij}(p,q)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Then for the entire **Kernals** matrix $\\frac{\\partial e_{ij}}{\\partial K^{(k)}}$\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial e_{ij}}{\\partial K^{(k)}} &= -2\\Big[(\\alpha^{\\text{true}}_{ij}-\\alpha_{ij})a_k + (\\beta^{\\text{true}}_{ij}-\\beta_{ij})b_k\\Big]\\;Y_{ij} \\\\[5pt]\n",
        "&=\n",
        "{\\scriptsize\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial e_{ij}}{\\partial K^{(k)}_{11}}\n",
        "& \\dots\n",
        "& \\frac{\\partial e_{ij}}{\\partial K^{(k)}_{1q}}\n",
        "\\\\[4pt]\n",
        "\\vdots\n",
        "& \\ddots\n",
        "& \\vdots\n",
        "\\\\[4pt]\n",
        "\\frac{\\partial e_{ij}}{\\partial K^{(k)}_{p1}}\n",
        "& \\dots\n",
        "& \\frac{\\partial e_{ij}}{\\partial K^{(k)}_{pq}}\n",
        "\\end{bmatrix}\n",
        "}\n",
        "\\\\[5pt]\n",
        "&=\n",
        "{\\scriptsize\n",
        "-2\\Big[(\\alpha^{\\text{true}}_{ij}-\\alpha_{ij})\\,a_k\n",
        "\\;+\\;\n",
        "(\\beta^{\\text{true}}_{ij}-\\beta_{ij})\\,b_k\\Big]\\;\n",
        "\\begin{bmatrix}\n",
        "Y_{ij}(1,1) & \\dots & Y_{ij}(1,q)\n",
        "\\\\[4pt]\n",
        "\\vdots & \\ddots & \\vdots\n",
        "\\\\[4pt]\n",
        "Y_{ij}(p,1) & \\dots & Y_{ij}(p,q)\n",
        "\\end{bmatrix}\n",
        "}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1-mM6kM39ZX"
      },
      "source": [
        "### **For total error**\n",
        "\n",
        "This is for one pixel error:\n",
        "$$\n",
        "\\frac{\\partial e_{ij}}{\\partial K^{(k)}} \\; = -2\\Big[(\\alpha^{\\text{true}}_{ij}-\\alpha_{ij})a_k + (\\beta^{\\text{true}}_{ij}-\\beta_{ij})b_k\\Big]\\;Y_{ij}\n",
        "$$\n",
        "\n",
        "for the total error we want to find $\\frac{\\partial E}{\\partial K^{(k)}}$\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "E\n",
        "&=\n",
        "\\sum_{i,j}\n",
        "\\|\\mathbf v_{ij}-\\widehat{\\mathbf v}_{ij}\\|^2 \\\\[4pt]\n",
        "&=\n",
        "\\|\\alpha^{\\text{true}} - Z\\mathbf a\\|^2\n",
        "+\\|\\beta^{\\text{true}} - Z\\mathbf b\\|^2.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Then\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial K^{(k)}}\n",
        "\\; = \\;\n",
        "\\sum_{i,j}\n",
        "\\frac{\\partial e_{ij}}{\\partial K^{(k)}}\n",
        "\\; = \\;\n",
        "-2\n",
        "\\sum_{i,j}\n",
        "\\Big[(\\alpha^{\\text{true}}_{ij}-\\alpha_{ij})a_k\n",
        "+ (\\beta^{\\text{true}}_{ij}-\\beta_{ij})b_k\\Big]\\;Y_{ij}.\n",
        "$$\n",
        "\n",
        "Thus the update rule for **gradient descent** can be described as\n",
        "\n",
        "$$\n",
        "K^{(k)}_{\\text{new}} = K^{(k)}_{\\text{old}} - c \\cdot \\frac{\\partial E}{\\partial K^{(k)}}\n",
        "$$\n",
        "\n",
        "where $c$ is our learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBZE7krKM1SQ"
      },
      "source": [
        "# **Activation Functions and non-linearity**\n",
        "\n",
        "Our current model is a lienear predictor\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} = \\mathbf a^\\top \\mathbf z_{ij}= a_1 z_1 + \\dots + a_m z_m,\n",
        "\\qquad\n",
        "\\beta_{ij} = \\mathbf b^\\top \\mathbf z_{ij} = b_1 z_1 + \\dots + b_m z_m\n",
        "$$\n",
        "\n",
        "**weighted sum** of features. Each features $a_i z_i \\in \\alpha_{ij}$ always contribute, There's no if certain features $z_k > \\text{certain value}$ the weight $a_k z_k$ should be $0$ (not contribute to $\\alpha_{ij}$.\n",
        "\n",
        "Thus it can't do\n",
        "\n",
        "$$\n",
        "z_k \\ge 0 \\implies a_k z_k = 0\n",
        "$$\n",
        "\n",
        "To fix this we can use the activation function **ReLU** activation function\n",
        "\n",
        "$$\n",
        "h_k = \\text{ReLU}(z_k)\n",
        "=\n",
        "\\begin{cases}\n",
        "0, & z_k \\le 0, \\\\[6pt]\n",
        "z_k, & z_k > 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "then prediction parameter $\\alpha_{ij}$ (same logic applies to $\\beta_{ij}$) becomes\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} = a_1 h_1 + a_2 h_2 + \\cdots + a_m h_m.\n",
        "$$\n",
        "\n",
        "Which is\n",
        "\n",
        "$$\n",
        "\\alpha_{ij}\n",
        "=\n",
        "a_1\\begin{cases}\n",
        "0, & z_1 \\le 0 \\\\\n",
        "z_1, & z_1 > 0\n",
        "\\end{cases}\n",
        "+\n",
        "a_2\\begin{cases}\n",
        "0, & z_2 \\le 0 \\\\\n",
        "z_2, & z_2 > 0\n",
        "\\end{cases} \\cdots\n",
        "$$\n",
        "\n",
        "Each term contributes:\n",
        "\n",
        "$$\n",
        "a_k h_k = \\begin{cases} 0, & z_k \\le 0 \\\\ a_k z_k, & z_k > 0 \\end{cases}\n",
        "$$\n",
        "\n",
        "So the whole $\\alpha_{ij}$ becomes:\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} =\n",
        "\\begin{cases} a_1 z_1 + a_2 z_2 + \\cdots + a_m z_m, & \\text{if all } z_k > 0, \\\\\n",
        " a_2 z_2 + a_4 z_4, & \\text{if  } z_2,z_4>0 \\text{ and others}\\le 0,\\\\\n",
        " a_3 z_3, & \\text{if  } z_3 > 0, \\\\\n",
        " 0, & \\text{if all } z_k \\le 0. \\end{cases}\n",
        " $$\n",
        "\n",
        "\n",
        "Now the network can **switch** between different linear rules depending on which features activate.\n",
        "\n",
        "\n",
        "Next consider looking at $z_k$ error contribution to $e_{\\alpha}$\n",
        "\n",
        "The chain rule is $K^{(k)}_{pq} \\rightarrow z_k \\rightarrow \\color{blue}{\\text{ReLU}(z_k)} \\color{black}{\\rightarrow} \\alpha_{ij} \\rightarrow e_{\\alpha}$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e_{\\alpha}}{\\partial K^{(k)}_{pq}} =\n",
        "\\frac{\\partial e_{\\alpha}}{\\partial \\alpha_{ij}}\n",
        "\\cdot\n",
        "\\frac{\\partial \\alpha_{ij}}{\\partial \\text{ReLU}(z_k)}\n",
        "\\cdot\n",
        "\\underbrace{\n",
        "\\color{blue}{\\frac{\\partial \\text{ReLU}(z_k)}{\\partial z_k}}\n",
        "}_{\\text{(New)}}\n",
        "\\cdot\n",
        "\\frac{\\partial z_k}{\\partial K^{(k)}_{pq}}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{ReLU}(z_k)}{\\partial z_k}\n",
        "=\n",
        "\\begin{cases}\n",
        "0, & z_k \\le 0 \\\\[4pt]\n",
        "1, & z_k > 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "If $h_k = \\text{ReLU}(z_k) = 0$, then the gradient $\\frac{\\partial e_{\\alpha}}{\\partial K^{(k)}*{pq}}$ also becomes zero. This means that **no portion of the total error (e*{\\alpha})** flows backward through the feature path $K^{(k)} \\rightarrow z_k \\rightarrow h_k$. In other words, feature $K^{(k)}$ contributed nothing to the prediction of $\\alpha_{ij}$, so it should not receive any blame or adjustment during gradient descent. This is exactly the desired behavior: when a feature is **inactive** (its ReLU output is zero), the network treats it as unused for that input, and therefore its filter $K^{(k)}$ is **not updated**, allowing CNNs to specialize different filters for different patterns.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrrjCTv5U1Ti"
      },
      "source": [
        "# **CNN Forward and Backpropagation**\n",
        "\n",
        "\n",
        "Let the input grayscale patch be $Y \\in \\mathbb{R}^{p\\times q}$.  \n",
        "A convolutional neural network with $L$ layers produces a sequence of feature\n",
        "vectors $(\\mathbf{h}^{(0)},\\mathbf{h}^{(1)},\\dots,\\mathbf{h}^{(L)})$,\n",
        "starting from\n",
        "\n",
        "$$\n",
        "\\mathbf{h}^{(0)} = Y.\n",
        "$$\n",
        "\n",
        "For each layer $\\ell = 1,\\dots,L$, let\n",
        "$K^{(\\ell,1)},\\dots,K^{(\\ell,m_\\ell)}$ be the learned convolution kernels.\n",
        "The pre-activation for feature $k$ in layer $\\ell$ is\n",
        "\n",
        "$$\n",
        "z^{(\\ell)}_k\n",
        "= \\langle K^{(\\ell,k)},\\,\\mathbf{h}^{(\\ell-1)}\\rangle,\n",
        "$$\n",
        "\n",
        "and the activation is ReLU:\n",
        "\n",
        "$$\n",
        "h^{(\\ell)}_k = \\mathrm{ReLU}(z^{(\\ell)}_k)\n",
        "=\n",
        "\\begin{cases}\n",
        "0, & z^{(\\ell)}_k \\le 0,\\\\[6pt]\n",
        "z^{(\\ell)}_k, & z^{(\\ell)}_k > 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The layer-$\\ell$ feature vector is\n",
        "\n",
        "$$\n",
        "\\mathbf{h}^{(\\ell)} =\n",
        "\\begin{bmatrix}\n",
        "h^{(\\ell)}_1\\\\[-2pt]\n",
        "\\vdots\\\\[-2pt]\n",
        "h^{(\\ell)}_{m_\\ell}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "After the final layer $L$, chroma coefficients are predicted by a linear\n",
        "output layer with parameters $\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^{m_L}$:\n",
        "\n",
        "$$\n",
        "\\alpha = \\mathbf{a}^\\top \\mathbf{h}^{(L)},\\qquad\n",
        "\\beta  = \\mathbf{b}^\\top \\mathbf{h}^{(L)}.\n",
        "$$\n",
        "\n",
        "Given “true’’ chroma coefficients $\\alpha^{\\text{true}},\\beta^{\\text{true}}$,\n",
        "we use the squared error\n",
        "\n",
        "$$\n",
        "e = (\\alpha^{\\text{true}} - \\alpha)^2\n",
        "  + (\\beta^{\\text{true}} - \\beta)^2.\n",
        "$$\n",
        "\n",
        "Backpropagation computes gradients of $e$ with respect to all parameters.  \n",
        "The derivatives at the output are\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial \\alpha}\n",
        "= -2(\\alpha^{\\text{true}} - \\alpha),\\qquad\n",
        "\\frac{\\partial e}{\\partial \\beta}\n",
        "= -2(\\beta^{\\text{true}} - \\beta).\n",
        "$$\n",
        "\n",
        "The gradients of the output weights are\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial a_k}\n",
        "= \\frac{\\partial e}{\\partial \\alpha}\\,h^{(L)}_k,\n",
        "\\qquad\n",
        "\\frac{\\partial e}{\\partial b_k}\n",
        "= \\frac{\\partial e}{\\partial \\beta}\\,h^{(L)}_k,\n",
        "$$\n",
        "\n",
        "and the error with respect to the top-layer activations is\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial h^{(L)}_k}\n",
        "=\n",
        "\\frac{\\partial e}{\\partial \\alpha}\\,a_k\n",
        "+\n",
        "\\frac{\\partial e}{\\partial \\beta}\\,b_k.\n",
        "$$\n",
        "\n",
        "For each hidden layer $\\ell = L,\\dots,1$ we define the pre-activation error\n",
        "\n",
        "$$\n",
        "\\delta^{(\\ell)}_k\n",
        ":= \\frac{\\partial e}{\\partial z^{(\\ell)}_k}\n",
        "= \\frac{\\partial e}{\\partial h^{(\\ell)}_k}\n",
        "\\cdot\n",
        "\\frac{\\partial h^{(\\ell)}_k}{\\partial z^{(\\ell)}_k}\n",
        "=\n",
        "\\begin{cases}\n",
        "0, & z^{(\\ell)}_k \\le 0,\\\\[6pt]\n",
        "\\displaystyle \\frac{\\partial e}{\\partial h^{(\\ell)}_k},\n",
        "& z^{(\\ell)}_k > 0,\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The gradient of the loss with respect to each kernel is\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial K^{(\\ell,k)}}\n",
        "=\n",
        "\\delta^{(\\ell)}_k\\,\\mathbf{h}^{(\\ell-1)}.\n",
        "$$\n",
        "\n",
        "The error on the previous layer’s activations is\n",
        "\n",
        "$$\n",
        "\\frac{\\partial e}{\\partial \\mathbf{h}^{(\\ell-1)}}\n",
        "=\n",
        "\\sum_{k=1}^{m_\\ell}\n",
        "\\delta^{(\\ell)}_k\\,K^{(\\ell,k)}.\n",
        "$$\n",
        "\n",
        "These are then used to compute the next layer’s error:\n",
        "\n",
        "$$\n",
        "\\delta^{(\\ell-1)}_r\n",
        "=\n",
        "\\frac{\\partial e}{\\partial h^{(\\ell-1)}_r}\n",
        "\\cdot\n",
        "\\frac{\\partial h^{(\\ell-1)}_r}{\\partial z^{(\\ell-1)}_r}\n",
        "=\n",
        "\\begin{cases}\n",
        "0, & z^{(\\ell-1)}_r \\le 0,\\\\[6pt]\n",
        "\\displaystyle \\frac{\\partial e}{\\partial h^{(\\ell-1)}_r},\n",
        "& z^{(\\ell-1)}_r > 0,\n",
        "\\end{cases}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hLzv3r4XFPg"
      },
      "source": [
        "# **Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "def rgb_to_lab_batch(imgs):  \n",
        "    B, _, H, W = imgs.shape\n",
        "    labs = []\n",
        "    for i in range(B):\n",
        "        rgb = imgs[i].permute(1,2,0).numpy().astype(np.float32)  \n",
        "        lab = cv2.cvtColor(rgb, cv2.COLOR_RGB2LAB)               \n",
        "    return torch.stack(labs, dim=0)  \n",
        "\n",
        "def lab_to_rgb_batch(L, ab):  \n",
        "    B, _, H, W = L.shape\n",
        "    rgbs = []\n",
        "    for i in range(B):\n",
        "        lab = torch.cat([L[i], ab[i]], dim=0).permute(1,2,0).numpy().astype(np.float32)  \n",
        "        rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
        "        rgbs.append(torch.from_numpy(np.clip(rgb, 0.0, 1.0)))\n",
        "    return torch.stack(rgbs, dim=0).permute(0,3,1,2)  \n",
        "\n",
        "\n",
        "def normalize_L(L):     \n",
        "    return L/50.0 - 1.0\n",
        "\n",
        "def denormalize_L(Ln): \n",
        "    return (Ln + 1.0) * 50.0\n",
        "\n",
        "def normalize_ab(ab):    \n",
        "    return ab / 128.0\n",
        "\n",
        "def denormalize_ab(abn): \n",
        "    return abn * 128.0\n",
        "\n",
        "# =========================================================\n",
        "#  CNN\n",
        "# =========================================================\n",
        "class ColorizationCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 2, 3, padding=1),\n",
        "            nn.Tanh()  # outputs in [-1,1] -> we'll scale to ab\n",
        "        )\n",
        "\n",
        "    def forward(self, L_norm):  # L_norm in [-1,1]\n",
        "        feats = self.encoder(L_norm)\n",
        "        ab_norm = self.decoder(feats)  # [-1,1]\n",
        "        return ab_norm\n",
        "\n",
        "# =========================================================\n",
        "#  CIFAR-10\n",
        "# =========================================================\n",
        "def load_cifar10(batch_size=128):\n",
        "    xform = transforms.ToTensor()  # returns [0,1]\n",
        "    train_ds = datasets.CIFAR10(\"./data\", train=True,  download=True, transform=xform)\n",
        "    test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=xform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# =========================================================\n",
        "#  Training / Eval\n",
        "# =========================================================\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for imgs, _ in loader:\n",
        "        labs = rgb_to_lab_batch(imgs)                   \n",
        "        L  = labs[..., 0].unsqueeze(1)                   \n",
        "        ab = labs[..., 1:3].permute(0,3,1,2).contiguous()\n",
        "        Ln  = normalize_L(L).to(device, non_blocking=True)        \n",
        "        abn = normalize_ab(ab).to(device, non_blocking=True)       \n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        abn_pred = model(Ln)                                      \n",
        "        loss = F.mse_loss(abn_pred, abn)                           \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    return total / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    for imgs, _ in loader:\n",
        "        labs = rgb_to_lab_batch(imgs)                  # CPU\n",
        "        L  = labs[..., 0].unsqueeze(1)                 # (B,1,H,W)\n",
        "        ab = labs[..., 1:3].permute(0,3,1,2).contiguous()\n",
        "\n",
        "        Ln  = normalize_L(L).to(device, non_blocking=True)\n",
        "        abn = normalize_ab(ab).to(device, non_blocking=True)\n",
        "\n",
        "        abn_pred = model(Ln)\n",
        "        loss = F.mse_loss(abn_pred, abn)\n",
        "        total += loss.item()\n",
        "    return total / len(loader)\n",
        "\n",
        "# =========================================================\n",
        "#  Visualization\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def show_predictions(model, loader, device, num_images=5, save_path=\"predictions_lab.png\"):\n",
        "    model.eval()\n",
        "    imgs, _ = next(iter(loader))  \n",
        "    labs = rgb_to_lab_batch(imgs)            \n",
        "    L  = labs[..., 0].unsqueeze(1)           \n",
        "    ab = labs[..., 1:3].permute(0,3,1,2)    \n",
        "\n",
        "    Ln = normalize_L(L).to(device)\n",
        "    abn_pred = model(Ln).cpu()\n",
        "    ab_pred = denormalize_ab(abn_pred)      \n",
        "\n",
        "    rgb_pred = lab_to_rgb_batch(L, ab_pred) \n",
        "    rgb_gt   = imgs                          \n",
        "    gray_vis = (L / 100.0).clamp(0,1)        \n",
        "\n",
        "    n = min(num_images, imgs.size(0))\n",
        "    fig, axes = plt.subplots(n, 3, figsize=(12, 3*n))\n",
        "    if n == 1:\n",
        "        axes = np.expand_dims(axes, 0)\n",
        "\n",
        "    for i in range(n):\n",
        "        axes[i,0].imshow(gray_vis[i,0].numpy(), cmap='gray', vmin=0, vmax=1)\n",
        "        axes[i,0].set_title(f\"L / grayscale {i+1}\")\n",
        "        axes[i,1].imshow(rgb_pred[i].permute(1,2,0).numpy())\n",
        "        axes[i,1].set_title(\"Predicted\")\n",
        "        axes[i,2].imshow(rgb_gt[i].permute(1,2,0).numpy())\n",
        "        axes[i,2].set_title(\"Ground Truth\")\n",
        "        for ax in axes[i]: ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    print(f\"Predictions saved to {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "# =========================================================\n",
        "#  Main\n",
        "# =========================================================\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    train_loader, test_loader = load_cifar10(batch_size=128)\n",
        "    model = ColorizationCNN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "    best = float('inf'); patience=5; bad=0\n",
        "    EPOCHS = 30\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        tr = train_one_epoch(model, train_loader, optimizer, device)\n",
        "        va = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {ep:02d}/{EPOCHS} | Train chroma MSE: {tr:.6f} | Val chroma MSE: {va:.6f}\")\n",
        "        if va < best:\n",
        "            best = va; bad = 0\n",
        "            torch.save(model.state_dict(), \"best_colorization_lab.pth\")\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience:\n",
        "                print(f\"Early stopping at epoch {ep}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_colorization_lab.pth\", map_location=device))\n",
        "    show_predictions(model, test_loader, device, num_images=5)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ======================\n",
        "#This is for Supervised Linear Regression(SLR). We manaully choose and apply kernals\n",
        "# ======================\n",
        "\n",
        "def get_kernels(device='cpu'):\n",
        "    \"\"\"Return normalized fixed kernels\"\"\"\n",
        "    kernels = []\n",
        "    kx, ky = cv2.getDerivKernels(1, 0, 3)\n",
        "    kernels.append(torch.from_numpy(kx * ky.T).float())\n",
        "    kx, ky = cv2.getDerivKernels(0, 1, 3)\n",
        "    kernels.append(torch.from_numpy(kx * ky.T).float())\n",
        "    kernels.append(torch.tensor([[0,1,0],[1,-4,1],[0,1,0]], dtype=torch.float32))  # Laplacian\n",
        "    kernels.append(torch.ones(4,4) / 16)                   # 4×4 average\n",
        "    g = cv2.getGaussianKernel(5, 1)                        # 5×5 Gaussian\n",
        "    kernels.append(torch.from_numpy(g @ g.T).float())\n",
        "    return [k / torch.norm(k) for k in kernels]\n",
        "\n",
        "\n",
        "# ======================\n",
        "#Exctract datas (RGB space): For both SLR and CNN\n",
        "# ======================\n",
        "def extract_datas(V_true, w, device='cpu'):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "        Z        : Feature matrix where Z[n] = z_{ij} for location (i,j)\n",
        "        V_target : True RGB values, v_{ij}^{true} for valid region\n",
        "        Y_target : True luminance values, y_{ij}^{true} for valid region\n",
        "    \"\"\"\n",
        "    H, W = V_true.shape[:2]\n",
        "    Y_true = torch.tensordot(V_true, w, dims=1)\n",
        "    # Convolve per kernel with reflect padding, then take the centered valid region\n",
        "    Ks = [k[None, None].to(device) for k in get_kernels(device)]  # [1,1,kH,kW] each\n",
        "    feature_maps = []\n",
        "    for k in Ks:\n",
        "        kh, kw = k.shape[2], k.shape[3]\n",
        "        pad_h, pad_w = kh // 2, kw // 2\n",
        "        Yp = F.pad(Y_true[None, None], (pad_w, pad_w, pad_h, pad_h), mode='reflect')\n",
        "        conv = F.conv2d(Yp, k, padding=0)[0, 0]  # [H', W']\n",
        "        # Per-kernel centered valid area (handles odd/even kernels)\n",
        "        sh = (kh - 1) // 2\n",
        "        eh = -(kh // 2) if kh > 1 else None\n",
        "        sw = (kw - 1) // 2\n",
        "        ew = -(kw // 2) if kw > 1 else None\n",
        "        fm = conv[sh:eh, sw:ew]\n",
        "        feature_maps.append(fm)\n",
        "\n",
        "    # Common dimension across all feature maps\n",
        "    min_h = min(fm.shape[0] for fm in feature_maps)\n",
        "    min_w = min(fm.shape[1] for fm in feature_maps)\n",
        "\n",
        "    # CENTER-CROP each feature map to (min_h, min_w)\n",
        "    cropped_features = []\n",
        "    for fm in feature_maps:\n",
        "        dh = (fm.shape[0] - min_h) // 2\n",
        "        dw = (fm.shape[1] - min_w) // 2\n",
        "        cropped_features.append(fm[dh:dh+min_h, dw:dw+min_w])\n",
        "\n",
        "    Z = torch.stack(cropped_features, dim=-1).reshape(-1, len(Ks))  # [N, K]\n",
        "    off_h = (H - min_h) // 2\n",
        "    off_w = (W - min_w) // 2\n",
        "    V_target = V_true[off_h:off_h+min_h, off_w:off_w+min_w].reshape(-1, 3)\n",
        "    Y_target = Y_true[off_h:off_h+min_h, off_w:off_w+min_w].reshape(-1)\n",
        "\n",
        "    return Z, V_target, Y_target, Y_true\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Core Formulation(Same for SLR and CNN etc)\n",
        "# ======================\n",
        "\n",
        "def compute_orthonormal_basis(w, device='cpu'):\n",
        "    w_norm = w / torch.norm(w)\n",
        "    tmp = torch.tensor([1., 0., 0.], device=device) if abs(w_norm[0]) < 0.9 else torch.tensor([0., 1., 0.], device=device)\n",
        "    u1 = tmp - w_norm * torch.dot(tmp, w_norm)\n",
        "    u1 = u1 / (torch.norm(u1) + 1e-8)\n",
        "    u2 = torch.linalg.cross(w_norm, u1)\n",
        "    u2 = u2 / (torch.norm(u2) + 1e-8)\n",
        "    return u1, u2\n",
        "\n",
        "# reconstructing rgb v = v_parralel + v_perp\n",
        "def reconstruct_color(Y_target, alpha, beta, w, u1, u2):\n",
        "    w2 = torch.dot(w, w)\n",
        "    v_parallel = (Y_target / w2).unsqueeze(1) * w\n",
        "    v_perp = alpha.unsqueeze(1) * u1 + beta.unsqueeze(1) * u2\n",
        "    return v_parallel + v_perp\n",
        "\n",
        "\n",
        "def SLR_OLS(Z, Y_target, V_target, w, u1, u2, device='cpu'):\n",
        "    \"\"\"\n",
        "    Supervised Linear Regression with OLS solution\n",
        "    \"\"\"\n",
        "    w2 = torch.dot(w, w)\n",
        "    v_parallel = (Y_target / w2).unsqueeze(1) * w\n",
        "    v_perp = V_target - v_parallel\n",
        "\n",
        "    # Train using OLS (closed-form solution)\n",
        "    a = torch.linalg.lstsq(Z, v_perp @ u1, rcond=-1).solution\n",
        "    b = torch.linalg.lstsq(Z, v_perp @ u2, rcond=-1).solution\n",
        "\n",
        "    return a, b  # Only return learned parameters\n",
        "\n",
        "# ======================\n",
        "# main pipeline\n",
        "# ======================\n",
        "\n",
        "def main():\n",
        "    # Test data\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    H, W = 64, 64\n",
        "    V_true = torch.rand(H, W, 3, device=device, dtype=torch.float32) #Radndomly create 64 x 64 RGB Tensor for test\n",
        "    w = torch.tensor([0.299, 0.587, 0.114], device=device)\n",
        "\n",
        "    # Extract data (for both SLR and CNN)\n",
        "    Z, V_target, Y_target, Y_true = extract_datas(V_true, w, device)\n",
        "\n",
        "    # Create orthonormal basis\n",
        "    u1, u2 = compute_orthonormal_basis(w, device)\n",
        "\n",
        "    # Train SLR model\n",
        "    a, b = SLR_OLS(Z, Y_target, V_target, w, u1, u2, device)\n",
        "\n",
        "    # Reconstruct (same for all methods)\n",
        "    alpha = Z @ a\n",
        "    beta = Z @ b\n",
        "    V_pred = reconstruct_color(Y_target, alpha, beta, w, u1, u2)\n",
        "    error = torch.mean((V_pred - V_target)**2).item()\n",
        "    print(f\"SLR Error: {error:.6f} | Feature shape: {Z.shape}\")\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
